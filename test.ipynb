{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openhgnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      " Basic setup of this experiment: \n",
      "     model: HMPNN    \n",
      "     dataset: OAG_CS   \n",
      "     task: ktn. \n",
      " This experiment has following parameters. You can use set_params to edit them.\n",
      " Use print(experiment) to print this information again.\n",
      "------------------------------------------------------------------------------\n",
      "batch_size: 64\n",
      "dataset_name: OAG_CS\n",
      "device: cpu\n",
      "evaluate_interval: 5\n",
      "gpu: -1\n",
      "hid_dim: 128\n",
      "hpo_search_space: None\n",
      "hpo_trials: 100\n",
      "in_dim: 1169\n",
      "load_from_pretrained: False\n",
      "lr: 0.0001\n",
      "matching_coeff: 1\n",
      "max_epoch: 200\n",
      "mini_batch_flag: False\n",
      "model_name: HMPNN\n",
      "num_layers: 4\n",
      "optimizer: Adam\n",
      "out_dim: 128\n",
      "output_dir: ./openhgnn/output/HMPNN\n",
      "patience: 1\n",
      "rel_dim: 128\n",
      "seed: 0\n",
      "source_type: paper\n",
      "target_type: author\n",
      "task_type: L1\n",
      "test_batch: 50\n",
      "train_batch: 200\n",
      "use_best_config: False\n",
      "use_matching_loss: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openhgnn import Experiment\n",
    "experiment = Experiment(model='HMPNN', dataset='OAG_CS', task='ktn', \n",
    "                        gpu=-1, lr=0.0001, \n",
    "                        max_epoch=200, num_layers=4,\n",
    "                        task_type='L1',source_type='paper',target_type='author',\n",
    "                        in_dim=1169,hid_dim=128,out_dim=128,rel_dim=128,\n",
    "                        batch_size=64, use_matching_loss=True,\n",
    "                        matching_coeff=1,mini_batch_flag=False,\n",
    "                        evaluate_interval=5\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13 Nov 16:34    INFO  [Config Info]\tModel: HMPNN,\tTask: ktn,\tDataset: OAG_CS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KTN_trainer\n",
      "starting ktn task\n",
      "Loading dataset oag_cs\n",
      "initing hmpnn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]13 Nov 16:35    INFO  Epoch 0 | full-batch training\n",
      "13 Nov 16:36    INFO  Epoch 0 | Train Loss 4.9279\n",
      "13 Nov 16:36    INFO  Start evaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.03131933377081966, 0.0008990592644013605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [01:12<3:59:28, 72.20s/it]13 Nov 16:36    INFO  Epoch 1 | full-batch training\n",
      "13 Nov 16:36    INFO  Epoch 1 | Train Loss 4.9170\n",
      "  1%|          | 2/200 [01:33<2:19:10, 42.18s/it]13 Nov 16:36    INFO  Epoch 2 | full-batch training\n",
      "13 Nov 16:36    INFO  Epoch 2 | Train Loss 4.9060\n",
      "  2%|▏         | 3/200 [01:54<1:47:33, 32.76s/it]13 Nov 16:36    INFO  Epoch 3 | full-batch training\n",
      "13 Nov 16:37    INFO  Epoch 3 | Train Loss 4.8950\n",
      "  2%|▏         | 4/200 [02:16<1:32:36, 28.35s/it]13 Nov 16:37    INFO  Epoch 4 | full-batch training\n",
      "13 Nov 16:37    INFO  Epoch 4 | Train Loss 4.8840\n",
      "  2%|▎         | 5/200 [02:38<1:24:21, 25.96s/it]13 Nov 16:37    INFO  Epoch 5 | full-batch training\n",
      "13 Nov 16:38    INFO  Epoch 5 | Train Loss 4.8730\n",
      "13 Nov 16:38    INFO  Start evaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1891581513077716, 0.00339889419055789)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [03:11<1:32:11, 28.51s/it]13 Nov 16:38    INFO  Epoch 6 | full-batch training\n",
      "13 Nov 16:38    INFO  Epoch 6 | Train Loss 4.8618\n",
      "  4%|▎         | 7/200 [03:32<1:23:52, 26.07s/it]13 Nov 16:38    INFO  Epoch 7 | full-batch training\n",
      "13 Nov 16:38    INFO  Epoch 7 | Train Loss 4.8503\n",
      "  4%|▍         | 8/200 [03:54<1:19:12, 24.75s/it]13 Nov 16:38    INFO  Epoch 8 | full-batch training\n",
      "13 Nov 16:39    INFO  Epoch 8 | Train Loss 4.8386\n",
      "  4%|▍         | 9/200 [04:15<1:15:16, 23.65s/it]13 Nov 16:39    INFO  Epoch 9 | full-batch training\n",
      "13 Nov 16:39    INFO  Epoch 9 | Train Loss 4.8267\n",
      "  5%|▌         | 10/200 [04:37<1:12:38, 22.94s/it]13 Nov 16:39    INFO  Epoch 10 | full-batch training\n",
      "13 Nov 16:39    INFO  Epoch 10 | Train Loss 4.8146\n",
      "13 Nov 16:39    INFO  Start evaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.27848348489109714, 0.0025073710710992545)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [05:10<1:21:48, 25.97s/it]13 Nov 16:40    INFO  Epoch 11 | full-batch training\n",
      "13 Nov 16:40    INFO  Epoch 11 | Train Loss 4.8022\n",
      "  6%|▌         | 12/200 [05:31<1:16:35, 24.44s/it]13 Nov 16:40    INFO  Epoch 12 | full-batch training\n",
      "13 Nov 16:40    INFO  Epoch 12 | Train Loss 4.7895\n",
      "  6%|▋         | 13/200 [05:52<1:13:27, 23.57s/it]13 Nov 16:40    INFO  Epoch 13 | full-batch training\n",
      "13 Nov 16:41    INFO  Epoch 13 | Train Loss 4.7763\n",
      "  7%|▋         | 14/200 [06:13<1:10:43, 22.81s/it]13 Nov 16:41    INFO  Epoch 14 | full-batch training\n",
      "13 Nov 16:41    INFO  Epoch 14 | Train Loss 4.7628\n",
      "  8%|▊         | 15/200 [06:34<1:08:50, 22.33s/it]13 Nov 16:41    INFO  Epoch 15 | full-batch training\n",
      "13 Nov 16:41    INFO  Epoch 15 | Train Loss 4.7487\n",
      "13 Nov 16:41    INFO  Start evaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2803115527639761, 0.0024783662816917195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [07:07<1:18:17, 25.53s/it]13 Nov 16:42    INFO  Epoch 16 | full-batch training\n",
      "13 Nov 16:42    INFO  Epoch 16 | Train Loss 4.7341\n",
      "  8%|▊         | 17/200 [07:28<1:13:39, 24.15s/it]13 Nov 16:42    INFO  Epoch 17 | full-batch training\n",
      "13 Nov 16:42    INFO  Epoch 17 | Train Loss 4.7189\n",
      "  9%|▉         | 18/200 [07:50<1:10:49, 23.35s/it]13 Nov 16:42    INFO  Epoch 18 | full-batch training\n",
      "13 Nov 16:43    INFO  Epoch 18 | Train Loss 4.7030\n",
      " 10%|▉         | 19/200 [08:11<1:08:36, 22.74s/it]13 Nov 16:43    INFO  Epoch 19 | full-batch training\n",
      "13 Nov 16:43    INFO  Epoch 19 | Train Loss 4.6865\n",
      " 10%|█         | 20/200 [08:33<1:07:07, 22.37s/it]13 Nov 16:43    INFO  Epoch 20 | full-batch training\n",
      "13 Nov 16:43    INFO  Epoch 20 | Train Loss 4.6693\n",
      "13 Nov 16:43    INFO  Start evaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.28032717727571016, 0.0024740374477142143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [09:06<1:16:14, 25.55s/it]13 Nov 16:44    INFO  Epoch 21 | full-batch training\n",
      "13 Nov 16:44    INFO  Epoch 21 | Train Loss 4.6514\n",
      " 11%|█         | 22/200 [09:27<1:12:14, 24.35s/it]13 Nov 16:44    INFO  Epoch 22 | full-batch training\n",
      "13 Nov 16:44    INFO  Epoch 22 | Train Loss 4.6327\n",
      " 12%|█▏        | 23/200 [09:49<1:09:16, 23.48s/it]13 Nov 16:44    INFO  Epoch 23 | full-batch training\n",
      "13 Nov 16:45    INFO  Epoch 23 | Train Loss 4.6131\n",
      " 12%|█▏        | 24/200 [10:09<1:06:16, 22.60s/it]13 Nov 16:45    INFO  Epoch 24 | full-batch training\n",
      "13 Nov 16:45    INFO  Epoch 24 | Train Loss 4.5928\n",
      " 12%|█▎        | 25/200 [10:30<1:04:33, 22.13s/it]13 Nov 16:45    INFO  Epoch 25 | full-batch training\n",
      "13 Nov 16:45    INFO  Epoch 25 | Train Loss 4.5715\n",
      "13 Nov 16:45    INFO  Start evaling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.28033498953157715, 0.0024740611060826436)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26/200 [11:03<1:13:17, 25.27s/it]13 Nov 16:46    INFO  Epoch 26 | full-batch training\n",
      "13 Nov 16:46    INFO  Epoch 26 | Train Loss 4.5494\n",
      " 14%|█▎        | 27/200 [11:24<1:09:19, 24.05s/it]13 Nov 16:46    INFO  Epoch 27 | full-batch training\n",
      "13 Nov 16:46    INFO  Epoch 27 | Train Loss 4.5263\n",
      " 14%|█▍        | 28/200 [11:45<1:06:11, 23.09s/it]13 Nov 16:46    INFO  Epoch 28 | full-batch training\n",
      "13 Nov 16:47    INFO  Epoch 28 | Train Loss 4.5022\n",
      " 14%|█▍        | 29/200 [12:06<1:04:08, 22.50s/it]13 Nov 16:47    INFO  Epoch 29 | full-batch training\n",
      "13 Nov 16:47    INFO  Epoch 29 | Train Loss 4.4771\n",
      " 15%|█▌        | 30/200 [12:27<1:02:29, 22.06s/it]13 Nov 16:47    INFO  Epoch 30 | full-batch training\n",
      "13 Nov 16:47    INFO  Epoch 30 | Train Loss 4.4509\n",
      "13 Nov 16:47    INFO  Start evaling\n",
      " 15%|█▌        | 30/200 [12:54<1:13:11, 25.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lacuna42/OpenHGNN/test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B104.155.157.37/home/lacuna42/OpenHGNN/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m experiment\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/OpenHGNN/openhgnn/experiment.py:128\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     flow \u001b[39m=\u001b[39m build_flow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, trainerflow)\n\u001b[0;32m--> 128\u001b[0m     result \u001b[39m=\u001b[39m flow\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39m'\u001b[39m\u001b[39mline_profiler_func\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    130\u001b[0m         prof\u001b[39m.\u001b[39mprint_stats()\n",
      "File \u001b[0;32m~/OpenHGNN/openhgnn/trainerflow/KTN_trainer.py:141\u001b[0m, in \u001b[0;36mKTNTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mStart evaling\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m     acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_full_test_step()\n\u001b[1;32m    142\u001b[0m     \u001b[39mprint\u001b[39m(acc)\n",
      "File \u001b[0;32m~/OpenHGNN/openhgnn/trainerflow/KTN_trainer.py:218\u001b[0m, in \u001b[0;36mKTNTrainer._full_test_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    217\u001b[0m     h_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\u001b[39m.\u001b[39mndata[\u001b[39m\"\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 218\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mg, h_dict)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type]\n\u001b[1;32m    219\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(logits)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_train_idx]\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask\u001b[39m.\u001b[39mevaluate(pred_y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_labels)\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OpenHGNN/openhgnn/models/HMPNN.py:35\u001b[0m, in \u001b[0;36mHMPNN.forward\u001b[0;34m(self, hg, h_dict)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(hg, \u001b[39m\"\u001b[39m\u001b[39mntypes\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 35\u001b[0m         h_dict \u001b[39m=\u001b[39m layer(hg, h_dict)\n\u001b[1;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OpenHGNN/openhgnn/models/HMPNN.py:74\u001b[0m, in \u001b[0;36mHMPNNLayer.forward\u001b[0;34m(self, g, h_dict)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, h_dict):\n\u001b[1;32m     73\u001b[0m     \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mlocal_scope():\n\u001b[0;32m---> 74\u001b[0m         h_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(g, h_dict)\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m h_dict\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/nn/pytorch/hetero.py:210\u001b[0m, in \u001b[0;36mHeteroGraphConv.forward\u001b[0;34m(self, g, inputs, mod_args, mod_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m stype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m inputs:\n\u001b[1;32m    209\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m         dstdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module((stype, etype, dtype))(\n\u001b[1;32m    211\u001b[0m             rel_graph,\n\u001b[1;32m    212\u001b[0m             (inputs[stype], inputs[dtype]),\n\u001b[1;32m    213\u001b[0m             \u001b[39m*\u001b[39;49mmod_args\u001b[39m.\u001b[39;49mget(etype, ()),\n\u001b[1;32m    214\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmod_kwargs\u001b[39m.\u001b[39;49mget(etype, {})\n\u001b[1;32m    215\u001b[0m         )\n\u001b[1;32m    216\u001b[0m         outputs[dtype]\u001b[39m.\u001b[39mappend(dstdata)\n\u001b[1;32m    217\u001b[0m rsts \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/nn/pytorch/conv/graphconv.py:457\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[39m# aggregate first then mult W\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     graph\u001b[39m.\u001b[39msrcdata[\u001b[39m\"\u001b[39m\u001b[39mh\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m feat_src\n\u001b[0;32m--> 457\u001b[0m     graph\u001b[39m.\u001b[39;49mupdate_all(aggregate_fn, fn\u001b[39m.\u001b[39;49msum(msg\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m\"\u001b[39;49m, out\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mh\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    458\u001b[0m     rst \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mdstdata[\u001b[39m\"\u001b[39m\u001b[39mh\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/heterograph.py:5110\u001b[0m, in \u001b[0;36mDGLGraph.update_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func, etype)\u001b[0m\n\u001b[1;32m   5108\u001b[0m _, dtid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mmetagraph\u001b[39m.\u001b[39mfind_edge(etid)\n\u001b[1;32m   5109\u001b[0m g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m etype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m[etype]\n\u001b[0;32m-> 5110\u001b[0m ndata \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mmessage_passing(\n\u001b[1;32m   5111\u001b[0m     g, message_func, reduce_func, apply_node_func\n\u001b[1;32m   5112\u001b[0m )\n\u001b[1;32m   5113\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5114\u001b[0m     core\u001b[39m.\u001b[39mis_builtin(reduce_func)\n\u001b[1;32m   5115\u001b[0m     \u001b[39mand\u001b[39;00m reduce_func\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   5116\u001b[0m     \u001b[39mand\u001b[39;00m ndata\n\u001b[1;32m   5117\u001b[0m ):\n\u001b[1;32m   5118\u001b[0m     \u001b[39m# Replace infinity with zero for isolated nodes\u001b[39;00m\n\u001b[1;32m   5119\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ndata\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/core.py:398\u001b[0m, in \u001b[0;36mmessage_passing\u001b[0;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Invoke message passing computation on the whole graph.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m    Results from the message passing computation.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    392\u001b[0m     is_builtin(mfunc)\n\u001b[1;32m    393\u001b[0m     \u001b[39mand\u001b[39;00m is_builtin(rfunc)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m ):\n\u001b[1;32m    397\u001b[0m     \u001b[39m# invoke fused message passing\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     ndata \u001b[39m=\u001b[39m invoke_gspmm(g, mfunc, rfunc)\n\u001b[1;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     \u001b[39m# invoke message passing in two separate steps\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[39m# message phase\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m is_builtin(mfunc):\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/core.py:368\u001b[0m, in \u001b[0;36minvoke_gspmm\u001b[0;34m(graph, mfunc, rfunc, srcdata, dstdata, edata)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[39melse\u001b[39;00m:  \u001b[39m# \"copy_e\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m             x \u001b[39m=\u001b[39m data_dict_to_list(graph, x, mfunc, \u001b[39m\"\u001b[39m\u001b[39me\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 368\u001b[0m     z \u001b[39m=\u001b[39m op(graph, x)\n\u001b[1;32m    369\u001b[0m \u001b[39mreturn\u001b[39;00m {rfunc\u001b[39m.\u001b[39mout_field: z}\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/ops/spmm.py:215\u001b[0m, in \u001b[0;36m_gen_copy_reduce_func.<locals>.func\u001b[0;34m(g, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(g, x):\n\u001b[1;32m    214\u001b[0m     \u001b[39mif\u001b[39;00m binary_op \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcopy_u\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39mreturn\u001b[39;00m gspmm(g, \u001b[39m\"\u001b[39;49m\u001b[39mcopy_lhs\u001b[39;49m\u001b[39m\"\u001b[39;49m, reduce_op, x, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    216\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[39mreturn\u001b[39;00m gspmm(g, \u001b[39m\"\u001b[39m\u001b[39mcopy_rhs\u001b[39m\u001b[39m\"\u001b[39m, reduce_op, \u001b[39mNone\u001b[39;00m, x)\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/ops/spmm.py:79\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(g, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m     77\u001b[0m         lhs_data, rhs_data \u001b[39m=\u001b[39m reshape_lhs_rhs(lhs_data, rhs_data)\n\u001b[1;32m     78\u001b[0m     \u001b[39m# With max and min reducers infinity will be returned for zero degree nodes\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[39m=\u001b[39m gspmm_internal(\n\u001b[1;32m     80\u001b[0m         g\u001b[39m.\u001b[39;49m_graph,\n\u001b[1;32m     81\u001b[0m         op,\n\u001b[1;32m     82\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m reduce_op \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39melse\u001b[39;49;00m reduce_op,\n\u001b[1;32m     83\u001b[0m         lhs_data,\n\u001b[1;32m     84\u001b[0m         rhs_data,\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# lhs_data or rhs_data is None only in unary functions like ``copy-u`` or ``copy_e``\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     lhs_data \u001b[39m=\u001b[39m (\n\u001b[1;32m     89\u001b[0m         [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m g\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mnumber_of_ntypes()\n\u001b[1;32m     90\u001b[0m         \u001b[39mif\u001b[39;00m lhs_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[39melse\u001b[39;00m lhs_data\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py:1032\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m   1030\u001b[0m args \u001b[39m=\u001b[39m _cast_if_autocast_enabled(gidx, op, reduce_op, lhs_data, rhs_data)\n\u001b[1;32m   1031\u001b[0m \u001b[39mwith\u001b[39;00m _disable_autocast_if_enabled():\n\u001b[0;32m-> 1032\u001b[0m     \u001b[39mreturn\u001b[39;00m GSpMM\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/backend/pytorch/sparse.py:165\u001b[0m, in \u001b[0;36mGSpMM.forward\u001b[0;34m(ctx, gidx, op, reduce_op, X, Y)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, gidx, op, reduce_op, X, Y):\n\u001b[0;32m--> 165\u001b[0m     out, (argX, argY) \u001b[39m=\u001b[39m _gspmm(gidx, op, reduce_op, X, Y)\n\u001b[1;32m    166\u001b[0m     reduce_last \u001b[39m=\u001b[39m _need_reduce_last_dim(X, Y)\n\u001b[1;32m    167\u001b[0m     X_shape \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/t/lib/python3.11/site-packages/dgl/_sparse_ops.py:239\u001b[0m, in \u001b[0;36m_gspmm\u001b[0;34m(gidx, op, reduce_op, u, e)\u001b[0m\n\u001b[1;32m    237\u001b[0m arg_e_nd \u001b[39m=\u001b[39m to_dgl_nd_for_write(arg_e)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m gidx\u001b[39m.\u001b[39mnum_edges(\u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 239\u001b[0m     _CAPI_DGLKernelSpMM(\n\u001b[1;32m    240\u001b[0m         gidx,\n\u001b[1;32m    241\u001b[0m         op,\n\u001b[1;32m    242\u001b[0m         reduce_op,\n\u001b[1;32m    243\u001b[0m         to_dgl_nd(u \u001b[39mif\u001b[39;49;00m use_u \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    244\u001b[0m         to_dgl_nd(e \u001b[39mif\u001b[39;49;00m use_e \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    245\u001b[0m         to_dgl_nd_for_write(v),\n\u001b[1;32m    246\u001b[0m         arg_u_nd,\n\u001b[1;32m    247\u001b[0m         arg_e_nd,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[39m# NOTE(zihao): actually we can avoid the following step, because arg_*_nd\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# refers to the data that stores arg_*. After we call _CAPI_DGLKernelSpMM,\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m# arg_* should have already been changed. But we found this doesn't work\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m# The workaround is proposed by Jinjing, and we still need to investigate\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# where the problem is.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m arg_u \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m arg_u \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m F\u001b[39m.\u001b[39mzerocopy_from_dgl_ndarray(arg_u_nd)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
